{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf62c4c7",
   "metadata": {},
   "source": [
    "# YourCabs Cancellation Prediction - Training Different Models\n",
    "\n",
    "Let's try training multiple ML models to see which one works best for predicting cab cancellations. I want to test a bunch of different approaches:\n",
    "\n",
    "- Linear Regression (just to see how it performs)\n",
    "- Logistic Regression \n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost (if I can get it working)\n",
    "\n",
    "Not sure which will work best, so we try them all and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002584b6",
   "metadata": {},
   "source": [
    "## Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fbc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, mean_squared_error, r2_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # too many warnings otherwise\n",
    "\n",
    "# trying to make plots look decent\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"imported everything\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e99d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to import my preprocessing function\n",
    "sys.path.append('../src')\n",
    "from preprocess import preprocess_data\n",
    "\n",
    "print(\"got the preprocessing function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e7e9e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd53b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    print(\"loading data...\")\n",
    "    df = pd.read_csv('../data/YourCabs.csv')\n",
    "    \n",
    "    print(\"preprocessing...\")\n",
    "    processed_df = preprocess_data(df)\n",
    "    \n",
    "    # split into features and target\n",
    "    X = processed_df.drop('Car_Cancellation', axis=1)\n",
    "    y = processed_df['Car_Cancellation']\n",
    "    \n",
    "    print(f\"got {X.shape[0]} samples with {X.shape[1]} features\")\n",
    "    print(f\"target split: {y.value_counts(normalize=True).to_dict()}\")\n",
    "    \n",
    "    return X, y, processed_df\n",
    "\n",
    "# load it up\n",
    "X, y, processed_df = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115fc089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what we're working with\n",
    "print(\"Dataset shape:\", processed_df.shape)\n",
    "print(\"\\nFeatures:\", list(X.columns))\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d46a8d",
   "metadata": {},
   "source": [
    "## Setting up the models\n",
    "\n",
    "Going to try a bunch of different models and see what works best. Some need scaling, some don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_configurations():\n",
    "    # just setting up all the models I want to try\n",
    "    # some might work better than others, we'll see\n",
    "    \n",
    "    models = {\n",
    "        # trying linear regression first as baseline\n",
    "        'Linear Regression': {\n",
    "            'model': LinearRegression(),\n",
    "            'needs_scaling': True,\n",
    "            'task_type': 'regression',\n",
    "            'notes': 'baseline model, probably not great for classification but whatever'\n",
    "        },\n",
    "        \n",
    "        # logistic regression should work better for classification\n",
    "        'Logistic Regression': {\n",
    "            'model': LogisticRegression(\n",
    "                random_state=42, \n",
    "                class_weight='balanced',  # since data might be imbalanced\n",
    "                max_iter=1000,\n",
    "                solver='liblinear'\n",
    "            ),\n",
    "            'needs_scaling': True,\n",
    "            'task_type': 'classification',\n",
    "            'notes': 'should give good probability estimates'\n",
    "        },\n",
    "        \n",
    "        # decision tree - easy to interpret\n",
    "        'Decision Tree': {\n",
    "            'model': DecisionTreeClassifier(\n",
    "                random_state=42, \n",
    "                class_weight='balanced',\n",
    "                max_depth=8,  # don't want it too deep\n",
    "                min_samples_split=50,\n",
    "                min_samples_leaf=20\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'task_type': 'classification',\n",
    "            'notes': 'should be easy to understand what it\\'s doing'\n",
    "        },\n",
    "        \n",
    "        # random forest usually works pretty well\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestClassifier(\n",
    "                n_estimators=100, \n",
    "                random_state=42, \n",
    "                class_weight='balanced',\n",
    "                max_depth=10,\n",
    "                min_samples_split=20,\n",
    "                min_samples_leaf=10\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'task_type': 'classification',\n",
    "            'notes': 'usually performs well, gives feature importance'\n",
    "        },\n",
    "        \n",
    "        # XGBoost usually wins competitions\n",
    "        'XGBoost': {\n",
    "            'model': XGBClassifier(\n",
    "                random_state=42,\n",
    "                eval_metric='logloss',\n",
    "                n_estimators=100,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8\n",
    "            ),\n",
    "            'needs_scaling': False,\n",
    "            'task_type': 'classification',\n",
    "            'notes': 'usually wins competitions, worth a try'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return models\n",
    "\n",
    "# get the models ready\n",
    "models = get_model_configurations()\n",
    "print(f\"set up {len(models)} models:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d6eb0c",
   "metadata": {},
   "source": [
    "## Split data and prep scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26537e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"training set: {X_train.shape[0]} samples\")\n",
    "print(f\"test set: {X_test.shape[0]} samples\")\n",
    "print(f\"train target split: {y_train.value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"test target split: {y_test.value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some models need scaled data, so let's prep that\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"scaling done\")\n",
    "print(f\"scaled train shape: {X_train_scaled.shape}\")\n",
    "print(f\"scaled test shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1a4732",
   "metadata": {},
   "source": [
    "## Training time\n",
    "\n",
    "Let's run through each model and see how they do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store results here\n",
    "results = {}\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(\"starting training...\")\n",
    "print(\"=\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ab4697",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Just trying this as a baseline, even though it's not really meant for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Linear Regression' in models:\n",
    "    print(\"trying linear regression...\")\n",
    "    \n",
    "    # needs scaled data\n",
    "    X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    \n",
    "    # train it\n",
    "    model = models['Linear Regression']['model']\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    # predictions\n",
    "    y_pred = model.predict(X_te)\n",
    "    \n",
    "    # metrics for regression\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # save results\n",
    "    results['Linear Regression'] = {\n",
    "        'model': model,\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'y_pred': y_pred,\n",
    "        'needs_scaling': True\n",
    "    }\n",
    "    \n",
    "    print(f\"MSE: {mse:.4f}, R²: {r2:.4f}\")\n",
    "    if r2 < 0:\n",
    "        print(\"ouch, negative R² means it's worse than just predicting the mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5babe5f",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "This should work much better for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e00d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Logistic Regression' in models:\n",
    "    print(\"training logistic regression...\")\n",
    "    \n",
    "    # also needs scaled data\n",
    "    X_tr, X_te = X_train_scaled, X_test_scaled\n",
    "    \n",
    "    model = models['Logistic Regression']['model']\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    # get predictions and probabilities\n",
    "    y_pred = model.predict(X_te)\n",
    "    y_pred_proba = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    # calculate metrics\n",
    "    accuracy = model.score(X_te, y_test)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # cross validation to be more sure\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_tr, y_train, \n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    results['Logistic Regression'] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'needs_scaling': True\n",
    "    }\n",
    "    \n",
    "    print(f\"accuracy: {accuracy:.3f}, AUC: {auc_score:.3f}\")\n",
    "    print(f\"CV AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0dfcde",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Let's try a decision tree - should be easy to interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Decision Tree' in models:\n",
    "    print(\"training decision tree...\")\n",
    "    \n",
    "    # doesn't need scaling\n",
    "    X_tr, X_te = X_train, X_test\n",
    "    \n",
    "    model = models['Decision Tree']['model']\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_te)\n",
    "    y_pred_proba = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    accuracy = model.score(X_te, y_test)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # cv again\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_tr, y_train, \n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    results['Decision Tree'] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'needs_scaling': False\n",
    "    }\n",
    "    \n",
    "    print(f\"accuracy: {accuracy:.3f}, AUC: {auc_score:.3f}\")\n",
    "    print(f\"CV AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    # decision trees can overfit easily\n",
    "    if cv_scores.std() > 0.05:\n",
    "        print(\"high CV variance - might be overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dda862",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "This usually works pretty well, let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800003b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Random Forest' in models:\n",
    "    print(\"training random forest...\")\n",
    "    \n",
    "    # also doesn't need scaling\n",
    "    X_tr, X_te = X_train, X_test\n",
    "    \n",
    "    model = models['Random Forest']['model']\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_te)\n",
    "    y_pred_proba = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    accuracy = model.score(X_te, y_test)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_tr, y_train, \n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    results['Random Forest'] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'needs_scaling': False\n",
    "    }\n",
    "    \n",
    "    print(f\"accuracy: {accuracy:.3f}, AUC: {auc_score:.3f}\")\n",
    "    print(f\"CV AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    # random forest is usually pretty stable\n",
    "    if cv_scores.mean() > 0.8:\n",
    "        print(\"nice! that's looking good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec48c12",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "If this works, it might be the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cffa2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'XGBoost' in models:\n",
    "    print(\"training XGBoost...\")\n",
    "    \n",
    "    # doesn't need scaling\n",
    "    X_tr, X_te = X_train, X_test\n",
    "    \n",
    "    model = models['XGBoost']['model']\n",
    "    model.fit(X_tr, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_te)\n",
    "    y_pred_proba = model.predict_proba(X_te)[:, 1]\n",
    "    \n",
    "    accuracy = model.score(X_te, y_test)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_tr, y_train, \n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='roc_auc'\n",
    "    )\n",
    "    \n",
    "    results['XGBoost'] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc_score,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'needs_scaling': False\n",
    "    }\n",
    "    \n",
    "    print(f\"accuracy: {accuracy:.3f}, AUC: {auc_score:.3f}\")\n",
    "    print(f\"CV AUC: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    if auc_score > 0.85:\n",
    "        print(\"wow XGBoost is killing it!\")\n",
    "    elif auc_score > 0.75:\n",
    "        print(\"XGBoost doing pretty well\")\n",
    "else:\n",
    "    print(\"no XGBoost, oh well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5f19c2",
   "metadata": {},
   "source": [
    "## Let's see how they all did\n",
    "\n",
    "Time to compare the results and see which model is actually worth using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e502d3e",
   "metadata": {},
   "source": [
    "### Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b2b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's put together all the results\n",
    "performance_data = []\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    if model_name == 'Linear Regression':  # different metrics for regression\n",
    "        performance_data.append({\n",
    "            'Model': model_name,\n",
    "            'MSE': result['mse'],\n",
    "            'R²': result['r2'],\n",
    "            'Task': 'Regression'\n",
    "        })\n",
    "    else:  # classification models\n",
    "        performance_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'AUC': result['auc'],\n",
    "            'CV_Mean': result['cv_mean'],\n",
    "            'CV_Std': result['cv_std'],\n",
    "            'Task': 'Classification'\n",
    "        })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "print(\"Results summary:\")\n",
    "print(performance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f229725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot classification model performance\n",
    "classification_models = performance_df[performance_df['Task'] == 'Classification']\n",
    "\n",
    "if len(classification_models) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    bars1 = ax1.bar(classification_models['Model'], classification_models['Accuracy'], \n",
    "                     color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # AUC plot\n",
    "    bars2 = ax2.bar(classification_models['Model'], classification_models['AUC'], \n",
    "                     color='lightcoral', alpha=0.7)\n",
    "    ax2.set_title('Model AUC Comparison', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('AUC Score')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac42f148",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd060f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(results, feature_names):\n",
    "    \"\"\"Analyze feature importance for tree-based models.\"\"\"\n",
    "    print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Using Random Forest for feature importance (most reliable ensemble method)\")\n",
    "    print()\n",
    "    \n",
    "    # Get feature importance from Random Forest (usually most reliable)\n",
    "    if 'Random Forest' in results:\n",
    "        rf_model = results['Random Forest']['model']\n",
    "        if hasattr(rf_model, 'feature_importances_'):\n",
    "            importances = rf_model.feature_importances_\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': feature_names,\n",
    "                'importance': importances\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(\"Top 10 Most Important Features (Random Forest):\")\n",
    "            for i, row in feature_importance.head(10).iterrows():\n",
    "                print(f\"  {row['feature']:25} {row['importance']:.4f}\")\n",
    "            \n",
    "            return feature_importance\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance = analyze_feature_importance(results, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994463cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "if feature_importance is not None:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(10)\n",
    "    \n",
    "    bars = plt.barh(range(len(top_features)), top_features['importance'], \n",
    "                    color='lightgreen', alpha=0.7)\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 10 Feature Importance (Random Forest)', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{width:.4f}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c76cb",
   "metadata": {},
   "source": [
    "### Model Rankings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41264581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_recommendations(results):\n",
    "    \"\"\"Print recommendations for each model.\"\"\"\n",
    "    print(\"MODEL RECOMMENDATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sort models by AUC score (better for imbalanced data)\n",
    "    model_scores = []\n",
    "    for name, result in results.items():\n",
    "        if name == 'Linear Regression':  # Skip regression model\n",
    "            continue\n",
    "        auc = result.get('auc', result['cv_mean'])  # Use AUC if available, else CV score\n",
    "        model_scores.append((name, auc, result))\n",
    "    \n",
    "    model_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Classification Models ranked by performance (AUC score):\")\n",
    "    for i, (name, score, result) in enumerate(model_scores, 1):\n",
    "        print(f\"\\n{i}. {name} (AUC: {score:.3f})\")\n",
    "        print(f\"   Needs scaling: {result['needs_scaling']}\")\n",
    "    \n",
    "    return model_scores\n",
    "\n",
    "# Print recommendations\n",
    "model_rankings = print_model_recommendations(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94c32da",
   "metadata": {},
   "source": [
    "### Detailed Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification reports for top 3 models\n",
    "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "top_3_models = model_rankings[:3] if len(model_rankings) >= 3 else model_rankings\n",
    "\n",
    "for i, (model_name, score, result) in enumerate(top_3_models, 1):\n",
    "    print(f\"\\n{i}. {model_name} (AUC: {score:.3f})\")\n",
    "    print(\"-\" * 40)\n",
    "    print(classification_report(y_test, result['y_pred'], target_names=['No Cancellation', 'Cancellation']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7221ab",
   "metadata": {},
   "source": [
    "### Confusion Matrices for Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92afe987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for top 3 models\n",
    "if len(model_rankings) > 0:\n",
    "    n_models = min(3, len(model_rankings))\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 4))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (model_name, score, result) in enumerate(model_rankings[:n_models]):\n",
    "        cm = confusion_matrix(y_test, result['y_pred'])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                   xticklabels=['No Cancel', 'Cancel'],\n",
    "                   yticklabels=['No Cancel', 'Cancel'],\n",
    "                   ax=axes[i])\n",
    "        axes[i].set_title(f'{model_name}\\n(AUC: {score:.3f})', fontweight='bold')\n",
    "        axes[i].set_xlabel('Predicted')\n",
    "        axes[i].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27521a",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "if model_rankings:\n",
    "    # Get the best model\n",
    "    best_model_name, best_score, best_result = model_rankings[0]\n",
    "    best_model = best_result['model']\n",
    "    \n",
    "    # Save the best model (joblib is good for models)\n",
    "    model_filename = f\"../models/best_model_{best_model_name.lower().replace(' ', '_')}.joblib\"\n",
    "    joblib.dump(best_model, model_filename)\n",
    "    \n",
    "    # Save the scaler if the model needs scaling\n",
    "    if best_result['needs_scaling']:\n",
    "        scaler_filename = f\"../models/scaler_{best_model_name.lower().replace(' ', '_')}.joblib\"\n",
    "        joblib.dump(scaler, scaler_filename)\n",
    "        print(f\"Saved scaler: {scaler_filename}\")\n",
    "    \n",
    "    print(f\"Saved best model ({best_model_name}): {model_filename}\")\n",
    "    print(f\"Best model AUC score: {best_score:.3f}\")\n",
    "    \n",
    "    # Save model metadata as JSON (much better than joblib for simple data)\n",
    "    metadata = {\n",
    "        'model_name': best_model_name,\n",
    "        'auc_score': float(best_score),  # ensure JSON serializable\n",
    "        'accuracy': float(best_result.get('accuracy', 0)),  # ensure JSON serializable\n",
    "        'needs_scaling': best_result['needs_scaling'],\n",
    "        'feature_names': feature_names,\n",
    "        'model_type': 'classification',\n",
    "        'training_date': '2025-07-15',  # could use datetime.now().isoformat()\n",
    "        'notes': 'Best performing model from comparison study'\n",
    "    }\n",
    "    \n",
    "    metadata_filename = f\"../models/model_metadata_{best_model_name.lower().replace(' ', '_')}.json\"\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)  # indent=2 makes it readable\n",
    "    print(f\"Saved model metadata: {metadata_filename}\")\n",
    "else:\n",
    "    print(\"No models to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8b2b0b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "So I trained a bunch of different models and compared their performance. Some worked better than others.\n",
    "\n",
    "The best model got saved automatically, so I can use that for making predictions later.\n",
    "\n",
    "Things I could try next:\n",
    "- Tune hyperparameters for the best model\n",
    "- Try feature engineering \n",
    "- Maybe ensemble a few models together\n",
    "- Actually deploy this thing somewhere\n",
    "\n",
    "For now I think this is good enough to see which approach works best for predicting cab cancellations.\n",
    "\n",
    "### How to Use the Saved Model:\n",
    "```python\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Load the model (joblib for models is fine)\n",
    "model = joblib.load('models/best_model_[name].joblib')\n",
    "\n",
    "# Load metadata (JSON is much better for simple data)\n",
    "with open('models/model_metadata_[name].json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# If scaling is needed\n",
    "if metadata['needs_scaling']:\n",
    "    scaler = joblib.load('models/scaler_[name].joblib')\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "    predictions = model.predict(X_new_scaled)\n",
    "else:\n",
    "    predictions = model.predict(X_new)\n",
    "\n",
    "# You can easily inspect metadata\n",
    "print(f\"Model: {metadata['model_name']}\")\n",
    "print(f\"AUC Score: {metadata['auc_score']}\")\n",
    "print(f\"Features: {metadata['feature_names']}\")\n",
    "```\n",
    "\n",
    "### Why JSON for metadata?\n",
    "- **Human readable** - you can open the .json file in any text editor\n",
    "- **Language agnostic** - works with any programming language\n",
    "- **Version control friendly** - easy to see changes in git\n",
    "- **Standard format** - universally supported"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
